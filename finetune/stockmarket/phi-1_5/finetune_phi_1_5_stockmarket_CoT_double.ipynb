{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl_FZ8DKoaPQ"
      },
      "source": [
        "# Preprocess data (Implementation derived from data preparation file by litgpt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Aj07zoSlZGsE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d0ffc91-23da-4958-f70d-d972efc20367"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan  4 17:03:30 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0              45W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CKiRfqb3wJOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad45acec-1d0d-43fa-abf2-fdeb071985f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "def install_dependencies():\n",
        "    !pip install -Uqq  git+https://github.com/huggingface/peft.git\n",
        "    !pip install -Uqq transformers datasets accelerate bitsandbytes\n",
        "    !pip install -Uqq wandb\n",
        "\n",
        "# uncomment the following line to install the required dependencies\n",
        "install_dependencies()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M8EAsMZpwfBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a07a8b5e-2cfb-466a-8a6b-4ed4c8e89cd8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('microsoft/phi-1_5',\n",
              " 'yc4142/stockmarket-CoT',\n",
              " 'phi-1_5-lora-int8-double-stockmarket-CoT',\n",
              " 'phi-1_5-lora-int8-double-stockmarket-CoT')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "model_name = (\"microsoft/phi-1_5\",\"microsoft/phi-1_5\")\n",
        "run_name = 'phi-1_5-lora-int8-double-stockmarket-CoT'\n",
        "dataset = 'yc4142/stockmarket-CoT'\n",
        "peft_name = 'phi-1_5-lora-int8-double-stockmarket-CoT'\n",
        "output_dir = 'phi-1_5-lora-int8-double-stockmarket-CoT-results'\n",
        "\n",
        "model_name[1],dataset,peft_name,run_name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report_to = \"wandb\" # \"none\"\n",
        "\n",
        "if report_to != \"none\":\n",
        "    import wandb\n",
        "    wandb.login()\n",
        "#04b000cbb7e17ddd83e391aa3996b53c7f064da7\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZa75SZqiM9i",
        "outputId": "901c82a6-eaa8-4d30-bf68-192eefc28608"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myc4142\u001b[0m (\u001b[33mcapstone_columbia\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=run_name,config={\n",
        "    \"model\": model_name[1],\n",
        "    \"dataset\":dataset\n",
        "})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "9diOxpIuiN_Z",
        "outputId": "02c05533-750b-4dcd-96f1-53474ef93605"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240104_170359-lfqhdn8v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/capstone_columbia/phi-1_5-lora-int8-double-stockmarket-CoT/runs/lfqhdn8v' target=\"_blank\">volcanic-water-2</a></strong> to <a href='https://wandb.ai/capstone_columbia/phi-1_5-lora-int8-double-stockmarket-CoT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/capstone_columbia/phi-1_5-lora-int8-double-stockmarket-CoT' target=\"_blank\">https://wandb.ai/capstone_columbia/phi-1_5-lora-int8-double-stockmarket-CoT</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/capstone_columbia/phi-1_5-lora-int8-double-stockmarket-CoT/runs/lfqhdn8v' target=\"_blank\">https://wandb.ai/capstone_columbia/phi-1_5-lora-int8-double-stockmarket-CoT/runs/lfqhdn8v</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/capstone_columbia/phi-1_5-lora-int8-double-stockmarket-CoT/runs/lfqhdn8v?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x79fd1b438280>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "#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\n"
      ],
      "metadata": {
        "id": "_1rqN7LIiO5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ba2dbd-df92-4a9e-d9d4-ec344dd24ca2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LnXI4gIDb6Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "repo_id = f'{huggingface_hub.whoami()[\"name\"]}/{peft_name}'\n",
        "print(repo_id)"
      ],
      "metadata": {
        "id": "ul1mKI-HiPjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb3ca7e-3d9a-4afc-be70-ddc1971572ce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yc4142/phi-1_5-lora-int8-double-stockmarket-CoT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CDHd96d1ofjn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "from transformers import AutoTokenizer\n",
        "import requests\n",
        "from datasets import Dataset\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "def prepare(\n",
        "    model_name:str = \"togethercomputer/RedPajama-INCITE-instruct-3B-v1\",\n",
        "    test_split_fraction: float = 0.03865,  # to get exactly 2000 test samples,\n",
        "    seed:int = 42,\n",
        "    dataset: str = \"yc4142/bias-CoT\",\n",
        "    ignore_index: int = 0,\n",
        "    max_seq_length:int = 2048,\n",
        ") -> list:\n",
        "    \"\"\"Prepare the Alpaca dataset for instruction tuning.\n",
        "\n",
        "    The output is a training and test dataset saved as `train.pt` and `test.pt`,\n",
        "    which stores the preprocessed and tokenized prompts and labels.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Partition the dataset into train and test\n",
        "    print(\"Loading dataset...\")\n",
        "    split_dataset = load_dataset(dataset, split='train').train_test_split(test_size=test_split_fraction, seed = seed)\n",
        "    train_set = split_dataset['train']\n",
        "    test_set = split_dataset['test']\n",
        "    print(\"example of original CoT data:\")\n",
        "\n",
        "    for key in train_set[3].keys():\n",
        "      print((key, train_set[3][key]))\n",
        "\n",
        "    print(f\"train has {len(train_set):,} samples\")\n",
        "    print(f\"test has {len(test_set):,} samples\")\n",
        "\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name,add_eos_token=True)\n",
        "    tokenizer.pad_token_id = 0\n",
        "    tokenizer.add_special_tokens({'eos_token':'<eos>'})\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Processing train split ...\")\n",
        "    train_data = []\n",
        "    for i in tqdm(range(0, len(train_set))):\n",
        "      train_data.append(prepare_sample(\n",
        "          example=train_set[i],\n",
        "          tokenizer=tokenizer,\n",
        "          max_length=max_seq_length,\n",
        "          ignore_index=ignore_index,\n",
        "          )\n",
        "      )\n",
        "\n",
        "   # train_set = [\n",
        "    #    prepare_sample(\n",
        "     #       example=sample,\n",
        "      #      tokenizer=tokenizer,\n",
        "       #     max_length=max_seq_length,\n",
        "        #    mask_inputs=mask_inputs,\n",
        "         #   ignore_index=ignore_index,\n",
        "        #)\n",
        "        #for sample in tqdm(train_set)\n",
        "    #]\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Processing test split ...\")\n",
        "    test_data = []\n",
        "    for i in tqdm(range(0, len(test_set))):\n",
        "      test_data.append(prepare_sample(\n",
        "          example=test_set[i],\n",
        "          tokenizer=tokenizer,\n",
        "          max_length=max_seq_length,\n",
        "          ignore_index=ignore_index,\n",
        "          )\n",
        "      )\n",
        "    #test_set = [\n",
        "     #   prepare_sample(\n",
        "      #      example=sample,\n",
        "       #     tokenizer=tokenizer,\n",
        "        #    max_length=max_seq_length,\n",
        "         #   mask_inputs=mask_inputs,\n",
        "          #  ignore_index=ignore_index,\n",
        "        #)\n",
        "        #for sample in tqdm(test_set)\n",
        "    #]\n",
        "\n",
        "    return tokenizer, Dataset.from_list(train_data), Dataset.from_list(test_data)\n",
        "\n",
        "\n",
        "\n",
        "def prepare_sample(example: dict, tokenizer: AutoTokenizer, max_length: int, ignore_index: int) -> dict:\n",
        "    \"\"\"Processes a single sample.\n",
        "\n",
        "    Each sample in the dataset consists of:\n",
        "    - instruction: A string describing the task\n",
        "    - input: A string holding a special input value for the instruction.\n",
        "        This only applies to some samples, and in others this is empty.\n",
        "    - output: The response string\n",
        "\n",
        "    This function processes this data to produce a prompt text and a label for\n",
        "    supervised training. The prompt text is formed as a single message including both\n",
        "    the instruction and the input. The label/target is the same message but with the\n",
        "    response attached.\n",
        "\n",
        "    Finally, both the prompt and the label get tokenized. If desired, all tokens\n",
        "    in the label that correspond to the original input prompt get masked out (default).\n",
        "    \"\"\"\n",
        "    full_prompt = generate_prompt(example)\n",
        "    full_prompt_and_response = full_prompt + example[\"output\"]\n",
        "    encoded_full_prompt = tokenizer(\n",
        "        full_prompt + \"<eos>\",  # add the end-of-stream token\n",
        "        truncation=True,\n",
        "        max_length= max_length,\n",
        "        padding=\"max_length\",\n",
        "        )\n",
        "    encoded_full_prompt_and_response = tokenizer(\n",
        "        full_prompt_and_response + \"<eos>\",\n",
        "        truncation=True,\n",
        "        max_length= max_length,\n",
        "        padding=\"max_length\",\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"instruction\": example[\"instruction\"],\n",
        "        \"input\": example[\"input\"],\n",
        "        \"output\": example[\"output\"],\n",
        "        \"input_ids\": encoded_full_prompt_and_response['input_ids'],\n",
        "        \"input_ids_no_response\": encoded_full_prompt['input_ids'],\n",
        "        \"labels\": encoded_full_prompt_and_response['input_ids'][1:]\n",
        "    }\n",
        "\n",
        "\n",
        "def generate_prompt(example: dict) -> str:\n",
        "    \"\"\"Generates a standardized message to prompt the model with an instruction, optional input and a\n",
        "    'response' field.\"\"\"\n",
        "\n",
        "    if example[\"input\"]:\n",
        "        return (\n",
        "            \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "            f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\"\n",
        "        )\n",
        "    return (\n",
        "        \"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nAd_vJWv4d3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31033c3-ce55-4798-ee2e-37b2e53220d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
            "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example of original CoT data:\n",
            "('output', \"People will have a neutral perspective about the stock given the headline “U.K.’s Largest Pension Fund Plans to Halve Hedge-Fund Holdings.” This is because from a financial market’s point of view, the impact of this action is largely balanced. \\n\\nLet's first understand the active participants here, the U.K.’s Largest Pension Fund and Hedge Funds. Pension funds are traditionally conservative investors that assemble vast pools of money to guarantee payments to employees once they retire. Hedge funds, on the other hand, are typically more aggressive in their investment strategies, often using leverage and derivatives in order to maximize returns. \\n\\nTo start with, when a pension fund reduces its exposure to hedge funds, it lessens its own volatility in the portfolio. Given that hedge funds often involve higher-risk strategies, a scaling back of such holdings could potentially reduce potential losses in scenarios of market volatility. From this viewpoint, such a move stands to fortify the overall stability of the investment ecosystem, which the market generally perceives with neutrality or positivity.\\n\\nHowever, on the flip side, the reduction of a significant investor's involvement in hedge funds – in this case, the U.K.'s largest pension fund – can also lead to a decrease in the demand for hedge fund-related stocks. This may have a downward pressure on the prices of these stocks. \\n\\nThat said, keep in mind the principle of risk and reward. While the hedge funds offer the potential for greater rewards, they do carry higher risk. So the pension fund's decision to halve its hedge fund investments could also mean a lower potential for both upside and downside, meaning less fluctuation, or volatility, in those associated stocks. \\n\\nThe above factors are likely to balance out, with the reduced demand applying downward pressure, and the reduced exposure to high-risk investments steadying the market, hence the expectation of stock prices remaining relatively constant. Given these expectations, market participants, coinciding with this sentiment, will likely hold their current positions, waiting for further clarity on future moves before initiating any significant transactions. The anticipation of stability encourages them to take no action, thereby maintaining the status quo and contributing to the neutral effect on stock prices.\")\n",
            "('instruction', 'Identify people\\'s perspective on stock market as a reaction to the following context. If you infer that people expect the stock price to go down, start your answer with \"People will have bearish perspective about the stock.\" If you infer that people expect the stock price to go up, start your answer with \"People will have bullish perspective about the stock.\" If you infer that people expect the stock price to stay constant, start your answer with \"People will have neutral perspective about the stock.\" ')\n",
            "('input', '\\n        context: U.K.’s Largest Pension Fund Plans to Halve Hedge-Fund Holdings\\n        ')\n",
            "train has 9,174 samples\n",
            "test has 369 samples\n",
            "Loading tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing train split ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9174/9174 [00:27<00:00, 334.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing test split ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 369/369 [00:01<00:00, 313.99it/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer, train_data, eval_data = prepare(\n",
        "    model_name = model_name[0],\n",
        "    test_split_fraction = 0.03865,\n",
        "    seed = 42,\n",
        "    dataset = dataset,\n",
        "    ignore_index = 0,\n",
        "    max_seq_length = 2048,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6CEOoJha8XNh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0758602d-1fd0-4d68-deb1-729fc48f6813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['instruction', 'input', 'output', 'input_ids', 'input_ids_no_response', 'labels'])\n",
            "('instruction', 'Identify people\\'s perspective on stock market as a reaction to the following context. If you infer that people expect the stock price to go down, start your answer with \"People will have bearish perspective about the stock.\" If you infer that people expect the stock price to go up, start your answer with \"People will have bullish perspective about the stock.\" If you infer that people expect the stock price to stay constant, start your answer with \"People will have neutral perspective about the stock.\" ')\n",
            "('input', '\\n        context: U.K.’s Largest Pension Fund Plans to Halve Hedge-Fund Holdings\\n        ')\n",
            "('output', \"People will have a neutral perspective about the stock given the headline “U.K.’s Largest Pension Fund Plans to Halve Hedge-Fund Holdings.” This is because from a financial market’s point of view, the impact of this action is largely balanced. \\n\\nLet's first understand the active participants here, the U.K.’s Largest Pension Fund and Hedge Funds. Pension funds are traditionally conservative investors that assemble vast pools of money to guarantee payments to employees once they retire. Hedge funds, on the other hand, are typically more aggressive in their investment strategies, often using leverage and derivatives in order to maximize returns. \\n\\nTo start with, when a pension fund reduces its exposure to hedge funds, it lessens its own volatility in the portfolio. Given that hedge funds often involve higher-risk strategies, a scaling back of such holdings could potentially reduce potential losses in scenarios of market volatility. From this viewpoint, such a move stands to fortify the overall stability of the investment ecosystem, which the market generally perceives with neutrality or positivity.\\n\\nHowever, on the flip side, the reduction of a significant investor's involvement in hedge funds – in this case, the U.K.'s largest pension fund – can also lead to a decrease in the demand for hedge fund-related stocks. This may have a downward pressure on the prices of these stocks. \\n\\nThat said, keep in mind the principle of risk and reward. While the hedge funds offer the potential for greater rewards, they do carry higher risk. So the pension fund's decision to halve its hedge fund investments could also mean a lower potential for both upside and downside, meaning less fluctuation, or volatility, in those associated stocks. \\n\\nThe above factors are likely to balance out, with the reduced demand applying downward pressure, and the reduced exposure to high-risk investments steadying the market, hence the expectation of stock prices remaining relatively constant. Given these expectations, market participants, coinciding with this sentiment, will likely hold their current positions, waiting for further clarity on future moves before initiating any significant transactions. The anticipation of stability encourages them to take no action, thereby maintaining the status quo and contributing to the neutral effect on stock prices.\")\n",
            "('input_ids', [21106, 318, 281, 12064, 326, 8477, 257, 4876, 11, 20312, 351, 281, 5128, 326, 3769, 2252, 4732, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 33234, 1958, 661, 338, 6650, 319, 4283, 1910, 355, 257, 6317, 284, 262, 1708, 4732, 13, 1002, 345, 13249, 326, 661, 1607, 262, 4283, 2756, 284, 467, 866, 11, 923, 534, 3280, 351, 366, 8061, 481, 423, 6842, 680, 6650, 546, 262, 4283, 526, 1002, 345, 13249, 326, 661, 1607, 262, 4283, 2756, 284, 467, 510, 11, 923, 534, 3280, 351, 366, 8061, 481, 423, 50079, 6650, 546, 262, 4283, 526, 1002, 345, 13249, 326, 661, 1607, 262, 4283, 2756, 284, 2652, 6937, 11, 923, 534, 3280, 351, 366, 8061, 481, 423, 8500, 6650, 546, 262, 4283, 526, 220, 198, 198, 21017, 23412, 25, 628, 50280, 22866, 25, 471, 13, 42, 13, 447, 247, 82, 406, 853, 395, 46391, 7557, 30305, 284, 11023, 303, 41078, 12, 24553, 31681, 198, 50280, 198, 198, 21017, 18261, 25, 8061, 481, 423, 257, 8500, 6650, 546, 262, 4283, 1813, 262, 16534, 564, 250, 52, 13, 42, 13, 447, 247, 82, 406, 853, 395, 46391, 7557, 30305, 284, 11023, 303, 41078, 12, 24553, 31681, 13, 447, 251, 770, 318, 780, 422, 257, 3176, 1910, 447, 247, 82, 966, 286, 1570, 11, 262, 2928, 286, 428, 2223, 318, 5688, 12974, 13, 220, 198, 198, 5756, 338, 717, 1833, 262, 4075, 6809, 994, 11, 262, 471, 13, 42, 13, 447, 247, 82, 406, 853, 395, 46391, 7557, 290, 41078, 34068, 13, 46391, 5153, 389, 16083, 5940, 7713, 326, 25432, 5909, 20354, 286, 1637, 284, 9149, 7524, 284, 4409, 1752, 484, 8058, 13, 41078, 5153, 11, 319, 262, 584, 1021, 11, 389, 6032, 517, 8361, 287, 511, 4896, 10064, 11, 1690, 1262, 16094, 290, 28486, 287, 1502, 284, 20487, 5860, 13, 220, 198, 198, 2514, 923, 351, 11, 618, 257, 13553, 1814, 12850, 663, 7111, 284, 19859, 5153, 11, 340, 1342, 641, 663, 898, 30772, 287, 262, 15320, 13, 11259, 326, 19859, 5153, 1690, 6211, 2440, 12, 19121, 10064, 11, 257, 20796, 736, 286, 884, 27572, 714, 6196, 4646, 2785, 9089, 287, 13858, 286, 1910, 30772, 13, 3574, 428, 28953, 11, 884, 257, 1445, 6296, 284, 6285, 1958, 262, 4045, 10159, 286, 262, 4896, 13187, 11, 543, 262, 1910, 4143, 36974, 1083, 351, 20723, 393, 1426, 11365, 13, 198, 198, 4864, 11, 319, 262, 14283, 1735, 11, 262, 7741, 286, 257, 2383, 15811, 338, 9750, 287, 19859, 5153, 784, 287, 428, 1339, 11, 262, 471, 13, 42, 2637, 82, 4387, 13553, 1814, 784, 460, 635, 1085, 284, 257, 10070, 287, 262, 3512, 329, 19859, 1814, 12, 5363, 14420, 13, 770, 743, 423, 257, 20841, 3833, 319, 262, 4536, 286, 777, 14420, 13, 220, 198, 198, 2504, 531, 11, 1394, 287, 2000, 262, 7989, 286, 2526, 290, 6721, 13, 2893, 262, 19859, 5153, 2897, 262, 2785, 329, 3744, 11530, 11, 484, 466, 3283, 2440, 2526, 13, 1406, 262, 13553, 1814, 338, 2551, 284, 10284, 303, 663, 19859, 1814, 11115, 714, 635, 1612, 257, 2793, 2785, 329, 1111, 17196, 290, 25320, 11, 3616, 1342, 19180, 2288, 11, 393, 30772, 11, 287, 883, 3917, 14420, 13, 220, 198, 198, 464, 2029, 5087, 389, 1884, 284, 5236, 503, 11, 351, 262, 5322, 3512, 11524, 20841, 3833, 11, 290, 262, 5322, 7111, 284, 1029, 12, 19121, 11115, 7978, 1112, 262, 1910, 11, 12891, 262, 17507, 286, 4283, 4536, 5637, 5365, 6937, 13, 11259, 777, 9027, 11, 1910, 6809, 11, 11194, 2530, 351, 428, 15598, 11, 481, 1884, 1745, 511, 1459, 6116, 11, 4953, 329, 2252, 16287, 319, 2003, 6100, 878, 40150, 597, 2383, 8945, 13, 383, 24054, 286, 10159, 18656, 606, 284, 1011, 645, 2223, 11, 12839, 10941, 262, 3722, 18658, 290, 14329, 284, 262, 8500, 1245, 319, 4283, 4536, 13, 50295, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "('input_ids_no_response', [21106, 318, 281, 12064, 326, 8477, 257, 4876, 11, 20312, 351, 281, 5128, 326, 3769, 2252, 4732, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 33234, 1958, 661, 338, 6650, 319, 4283, 1910, 355, 257, 6317, 284, 262, 1708, 4732, 13, 1002, 345, 13249, 326, 661, 1607, 262, 4283, 2756, 284, 467, 866, 11, 923, 534, 3280, 351, 366, 8061, 481, 423, 6842, 680, 6650, 546, 262, 4283, 526, 1002, 345, 13249, 326, 661, 1607, 262, 4283, 2756, 284, 467, 510, 11, 923, 534, 3280, 351, 366, 8061, 481, 423, 50079, 6650, 546, 262, 4283, 526, 1002, 345, 13249, 326, 661, 1607, 262, 4283, 2756, 284, 2652, 6937, 11, 923, 534, 3280, 351, 366, 8061, 481, 423, 8500, 6650, 546, 262, 4283, 526, 220, 198, 198, 21017, 23412, 25, 628, 50280, 22866, 25, 471, 13, 42, 13, 447, 247, 82, 406, 853, 395, 46391, 7557, 30305, 284, 11023, 303, 41078, 12, 24553, 31681, 198, 50280, 198, 198, 21017, 18261, 25, 50295, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "('labels', [318, 281, 12064, 326, 8477, 257, 4876, 11, 20312, 351, 281, 5128, 326, 3769, 2252, 4732, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 33234, 1958, 661, 338, 6650, 319, 4283, 1910, 355, 257, 6317, 284, 262, 1708, 4732, 13, 1002, 345, 13249, 326, 661, 1607, 262, 4283, 2756, 284, 467, 866, 11, 923, 534, 3280, 351, 366, 8061, 481, 423, 6842, 680, 6650, 546, 262, 4283, 526, 1002, 345, 13249, 326, 661, 1607, 262, 4283, 2756, 284, 467, 510, 11, 923, 534, 3280, 351, 366, 8061, 481, 423, 50079, 6650, 546, 262, 4283, 526, 1002, 345, 13249, 326, 661, 1607, 262, 4283, 2756, 284, 2652, 6937, 11, 923, 534, 3280, 351, 366, 8061, 481, 423, 8500, 6650, 546, 262, 4283, 526, 220, 198, 198, 21017, 23412, 25, 628, 50280, 22866, 25, 471, 13, 42, 13, 447, 247, 82, 406, 853, 395, 46391, 7557, 30305, 284, 11023, 303, 41078, 12, 24553, 31681, 198, 50280, 198, 198, 21017, 18261, 25, 8061, 481, 423, 257, 8500, 6650, 546, 262, 4283, 1813, 262, 16534, 564, 250, 52, 13, 42, 13, 447, 247, 82, 406, 853, 395, 46391, 7557, 30305, 284, 11023, 303, 41078, 12, 24553, 31681, 13, 447, 251, 770, 318, 780, 422, 257, 3176, 1910, 447, 247, 82, 966, 286, 1570, 11, 262, 2928, 286, 428, 2223, 318, 5688, 12974, 13, 220, 198, 198, 5756, 338, 717, 1833, 262, 4075, 6809, 994, 11, 262, 471, 13, 42, 13, 447, 247, 82, 406, 853, 395, 46391, 7557, 290, 41078, 34068, 13, 46391, 5153, 389, 16083, 5940, 7713, 326, 25432, 5909, 20354, 286, 1637, 284, 9149, 7524, 284, 4409, 1752, 484, 8058, 13, 41078, 5153, 11, 319, 262, 584, 1021, 11, 389, 6032, 517, 8361, 287, 511, 4896, 10064, 11, 1690, 1262, 16094, 290, 28486, 287, 1502, 284, 20487, 5860, 13, 220, 198, 198, 2514, 923, 351, 11, 618, 257, 13553, 1814, 12850, 663, 7111, 284, 19859, 5153, 11, 340, 1342, 641, 663, 898, 30772, 287, 262, 15320, 13, 11259, 326, 19859, 5153, 1690, 6211, 2440, 12, 19121, 10064, 11, 257, 20796, 736, 286, 884, 27572, 714, 6196, 4646, 2785, 9089, 287, 13858, 286, 1910, 30772, 13, 3574, 428, 28953, 11, 884, 257, 1445, 6296, 284, 6285, 1958, 262, 4045, 10159, 286, 262, 4896, 13187, 11, 543, 262, 1910, 4143, 36974, 1083, 351, 20723, 393, 1426, 11365, 13, 198, 198, 4864, 11, 319, 262, 14283, 1735, 11, 262, 7741, 286, 257, 2383, 15811, 338, 9750, 287, 19859, 5153, 784, 287, 428, 1339, 11, 262, 471, 13, 42, 2637, 82, 4387, 13553, 1814, 784, 460, 635, 1085, 284, 257, 10070, 287, 262, 3512, 329, 19859, 1814, 12, 5363, 14420, 13, 770, 743, 423, 257, 20841, 3833, 319, 262, 4536, 286, 777, 14420, 13, 220, 198, 198, 2504, 531, 11, 1394, 287, 2000, 262, 7989, 286, 2526, 290, 6721, 13, 2893, 262, 19859, 5153, 2897, 262, 2785, 329, 3744, 11530, 11, 484, 466, 3283, 2440, 2526, 13, 1406, 262, 13553, 1814, 338, 2551, 284, 10284, 303, 663, 19859, 1814, 11115, 714, 635, 1612, 257, 2793, 2785, 329, 1111, 17196, 290, 25320, 11, 3616, 1342, 19180, 2288, 11, 393, 30772, 11, 287, 883, 3917, 14420, 13, 220, 198, 198, 464, 2029, 5087, 389, 1884, 284, 5236, 503, 11, 351, 262, 5322, 3512, 11524, 20841, 3833, 11, 290, 262, 5322, 7111, 284, 1029, 12, 19121, 11115, 7978, 1112, 262, 1910, 11, 12891, 262, 17507, 286, 4283, 4536, 5637, 5365, 6937, 13, 11259, 777, 9027, 11, 1910, 6809, 11, 11194, 2530, 351, 428, 15598, 11, 481, 1884, 1745, 511, 1459, 6116, 11, 4953, 329, 2252, 16287, 319, 2003, 6100, 878, 40150, 597, 2383, 8945, 13, 383, 24054, 286, 10159, 18656, 606, 284, 1011, 645, 2223, 11, 12839, 10941, 262, 3722, 18658, 290, 14329, 284, 262, 8500, 1245, 319, 4283, 4536, 13, 50295, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "print(train_data[3].keys())\n",
        "for key in train_data[3].keys():\n",
        "  print((key, train_data[3][key]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BosoLBv3Ke3z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNLf_jOUGxsp"
      },
      "source": [
        "# Fine tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "25jmA705GzTx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8vkeRLHVJm2z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xdKycQxfhncd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05NvAx9PkLtw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "id": "dY7L_KbyydzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fcc802b-f8a0-4071-e478-e58da4266260"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ydEcSKCqJweV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df19143-44bb-4f30-d490-e63529d2f151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model for model:  microsoft/phi-1_5\n",
            "The repository for microsoft/phi-1_5 contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/phi-1_5.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n",
            "The repository for microsoft/phi-1_5 contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/phi-1_5.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "print(\"Loading model for model: \", model_name[0])\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_use_double_quant=True,\n",
        "    bnb_8bit_quant_type=\"nf8\",\n",
        "    bnb_8bit_compute_dtype=torch.float16\n",
        ")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name[0],\n",
        "    quantization_config = bnb_config,\n",
        "    device_map={\"\": 0},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "\n",
        "# Define LoRA Config\n",
        "lora_config = LoraConfig(\n",
        " r= 8,\n",
        " lora_alpha=16,\n",
        " target_modules=[\"Wqkv\"],\n",
        " lora_dropout=0.05,\n",
        " bias=\"none\",\n",
        " task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "# prepare int-8 model for training\n",
        "#model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# add LoRA adaptor\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "2pn8KBt3Mq0I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb78b85f-a7fb-4f2d-e388-d9a2b572fd2c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,572,864 || all params: 1,419,843,584 || trainable%: 0.11077727277316766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kLhnh5CzQTZf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "eval_steps = 100\n",
        "save_steps = 100\n",
        "logging_steps = 20\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset= eval_data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        num_train_epochs=5,\n",
        "        learning_rate=3e-4,\n",
        "        logging_steps=logging_steps,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        eval_steps=eval_steps,\n",
        "        save_steps=save_steps,\n",
        "        output_dir=output_dir,\n",
        "        report_to=report_to if report_to else \"none\",\n",
        "        save_total_limit=3,\n",
        "        load_best_model_at_end=True,\n",
        "        push_to_hub=False,\n",
        "        auto_find_batch_size=True,\n",
        "        warmup_steps=100,\n",
        "        weight_decay = 0.01,\n",
        "        per_device_train_batch_size = 2,\n",
        "        per_device_eval_batch_size = 2,\n",
        "        gradient_accumulation_steps = 32\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "id": "mgkz6-u-PvEz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "SVixs_h7Q7ta",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bfff662a-bea5-463c-bad9-1387670244d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1430' max='1430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1430/1430 7:54:05, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.792300</td>\n",
              "      <td>1.770854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.711400</td>\n",
              "      <td>1.700733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.688500</td>\n",
              "      <td>1.681693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.672900</td>\n",
              "      <td>1.671842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.668200</td>\n",
              "      <td>1.664704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.657600</td>\n",
              "      <td>1.663386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.651700</td>\n",
              "      <td>1.659803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.655200</td>\n",
              "      <td>1.655750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.649500</td>\n",
              "      <td>1.657915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.651300</td>\n",
              "      <td>1.651805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.654700</td>\n",
              "      <td>1.651650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.641200</td>\n",
              "      <td>1.653423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.643000</td>\n",
              "      <td>1.651957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.640200</td>\n",
              "      <td>1.651728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1430, training_loss=1.6936440981351413, metrics={'train_runtime': 28464.2896, 'train_samples_per_second': 1.611, 'train_steps_per_second': 0.05, 'total_flos': 7.394151460188979e+17, 'train_loss': 1.6936440981351413, 'epoch': 4.99})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S6ln8zQJivG9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.push_to_hub(repo_id)\n",
        "tokenizer.push_to_hub(repo_id)\n"
      ],
      "metadata": {
        "id": "QGjhOzKafXN1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293,
          "referenced_widgets": [
            "f94594955fc945cbaf5130226c9fb68d",
            "affa1c9f9ace4c8098eba79afdb33a1d",
            "07c1f90310294ca896d27fbe80577be2",
            "0dc28ceacc6541479ed83bfef5392189",
            "22f8d5ba2d1c45bcb94b1b570ea4e045",
            "af6084e8124040ecaac06cf9bc79e602",
            "78c009172a67451196107c7a1e621c08",
            "8a98d792236c4fe0b486319b17cca073",
            "0d4fa67d6ace47f3aa7ee9fbbfc7a2c8",
            "af9de155fd434a769171eda5202e4aa0",
            "2fb0386fcb11415aa9b99ae2672f4fab"
          ]
        },
        "outputId": "42a231d3-1e78-4ebe-fa59-1e0ead15d761"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/6.30M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f94594955fc945cbaf5130226c9fb68d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/yc4142/phi-1_5-lora-int8-double-stockmarket-CoT/commit/37b415e1ff961984aaac2178d1ee3e6791dde3d6', commit_message='Upload tokenizer', commit_description='', oid='37b415e1ff961984aaac2178d1ee3e6791dde3d6', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/Users/yuduc2/Desktop/JPmorgan project/LLMReasoningFinetuning/finetune/bias/phi-1_5/cot_8bit_singleQ\")"
      ],
      "metadata": {
        "id": "2cOYCsxvYyCZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vQC0QW9hBnzn"
      },
      "execution_count": 17,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f94594955fc945cbaf5130226c9fb68d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_affa1c9f9ace4c8098eba79afdb33a1d",
              "IPY_MODEL_07c1f90310294ca896d27fbe80577be2",
              "IPY_MODEL_0dc28ceacc6541479ed83bfef5392189"
            ],
            "layout": "IPY_MODEL_22f8d5ba2d1c45bcb94b1b570ea4e045"
          }
        },
        "affa1c9f9ace4c8098eba79afdb33a1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af6084e8124040ecaac06cf9bc79e602",
            "placeholder": "​",
            "style": "IPY_MODEL_78c009172a67451196107c7a1e621c08",
            "value": "adapter_model.safetensors: 100%"
          }
        },
        "07c1f90310294ca896d27fbe80577be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a98d792236c4fe0b486319b17cca073",
            "max": 6297552,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d4fa67d6ace47f3aa7ee9fbbfc7a2c8",
            "value": 6297552
          }
        },
        "0dc28ceacc6541479ed83bfef5392189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af9de155fd434a769171eda5202e4aa0",
            "placeholder": "​",
            "style": "IPY_MODEL_2fb0386fcb11415aa9b99ae2672f4fab",
            "value": " 6.30M/6.30M [00:02&lt;00:00, 4.40MB/s]"
          }
        },
        "22f8d5ba2d1c45bcb94b1b570ea4e045": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af6084e8124040ecaac06cf9bc79e602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78c009172a67451196107c7a1e621c08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a98d792236c4fe0b486319b17cca073": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d4fa67d6ace47f3aa7ee9fbbfc7a2c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af9de155fd434a769171eda5202e4aa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fb0386fcb11415aa9b99ae2672f4fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}