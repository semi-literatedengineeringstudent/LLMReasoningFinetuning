{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl_FZ8DKoaPQ"
      },
      "source": [
        "# Preprocess data (Implementation derived from data preparation file by litgpt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj07zoSlZGsE",
        "outputId": "b00c39b9-7f83-4c11-8234-b691c73c2fee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 14 10:51:58 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0              65W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKiRfqb3wJOU",
        "outputId": "3af84caf-717a-48d8-e073-aa0262166d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "def install_dependencies():\n",
        "  !pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
        "  !pip install -q datasets bitsandbytes einops wandb\n",
        "\n",
        "# uncomment the following line to install the required dependencies\n",
        "install_dependencies()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8EAsMZpwfBO",
        "outputId": "243dbd31-57ec-42e8-a5de-a7873de9753f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('togethercomputer/RedPajama-INCITE-Instruct-3B-v1',\n",
              " 'yc4142/stockmarket-CoT',\n",
              " 'RedPajama-INCITE-Instruct-3B-v1-lora-stockmarket-CoT',\n",
              " 'RedPajama-INCITE-Instruct-3B-v1-lora-stockmarket-CoT')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "model_name = (\"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\",\"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\")\n",
        "run_name = 'RedPajama-INCITE-Instruct-3B-v1-lora-int8-metaphor-nonCoT'\n",
        "dataset = 'JinchengLiu2000/Metaphot_non_CoT_ft'\n",
        "peft_name = 'RedPajama-INCITE-Instruct-3B-v1-lora-int8-metaphor-nonCoT'\n",
        "output_dir = 'RedPajama-INCITE-Instruct-3B-v1-lora-int8-metaphor-nonCoT-results'\n",
        "\n",
        "model_name[1],dataset,peft_name,run_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDHd96d1ofjn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "from transformers import AutoTokenizer\n",
        "import requests\n",
        "from datasets import Dataset\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAd_vJWv4d3p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CEOoJha8XNh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNLf_jOUGxsp"
      },
      "source": [
        "# Fine tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPVj1FDOUSfC"
      },
      "source": [
        "Get your wandb api key from https://wandb.ai/authorize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25jmA705GzTx",
        "outputId": "b0ccfe6a-1575-46a8-af87-5d18fb52bbf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myc4142\u001b[0m (\u001b[33mcapstone_columbia\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "report_to = \"wandb\" # \"none\"\n",
        "\n",
        "if report_to != \"none\":\n",
        "    import wandb\n",
        "    wandb.login()\n",
        "#7cc4240d81ca82afc28e021b820ba11f43bbcd3e\n",
        "#this is my wandb login api key, please paste yours from https://wandb.ai/authorize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "8vkeRLHVJm2z",
        "outputId": "cb7e938b-5929-4744-c5a1-6e593671b332"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240114_105223-5bo44v21</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/capstone_columbia/RedPajama-INCITE-Instruct-3B-v1-lora-stockmarket-CoT/runs/5bo44v21' target=\"_blank\">twilight-wildflower-2</a></strong> to <a href='https://wandb.ai/capstone_columbia/RedPajama-INCITE-Instruct-3B-v1-lora-stockmarket-CoT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/capstone_columbia/RedPajama-INCITE-Instruct-3B-v1-lora-stockmarket-CoT' target=\"_blank\">https://wandb.ai/capstone_columbia/RedPajama-INCITE-Instruct-3B-v1-lora-stockmarket-CoT</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/capstone_columbia/RedPajama-INCITE-Instruct-3B-v1-lora-stockmarket-CoT/runs/5bo44v21' target=\"_blank\">https://wandb.ai/capstone_columbia/RedPajama-INCITE-Instruct-3B-v1-lora-stockmarket-CoT/runs/5bo44v21</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/capstone_columbia/RedPajama-INCITE-Instruct-3B-v1-lora-stockmarket-CoT/runs/5bo44v21?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f4a3152bfa0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "wandb.init(project=run_name,config={\n",
        "    \"model\": model_name[1],\n",
        "    \"dataset\":dataset\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxoJcyTUUSfD"
      },
      "source": [
        "Log into your hugging face accout\n",
        "\n",
        "go to https://huggingface.co/settings/tokens, get an access token\n",
        "\n",
        "and create a model repo with name that is same as \"peft_name\" at the second cell of the notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI-wwl6GUSfD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdKycQxfhncd",
        "outputId": "9a449594-feb6-4b86-db1c-1e9c80b394c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login\n",
        "#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\n",
        "# make a repo with peft name on your hf account and your hf access token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05NvAx9PkLtw",
        "outputId": "132d2a87-03ab-44d2-8282-40027093f5cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yc4142/RedPajama-INCITE-Instruct-3B-v1-lora-stockmarket-CoT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import huggingface_hub\n",
        "repo_id = f'{huggingface_hub.whoami()[\"name\"]}/{peft_name}'\n",
        "print(repo_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydEcSKCqJweV",
        "outputId": "98457a79-a341-4ac3-a60b-1f8e4dc1ede8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model for model:  togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "print(\"Loading model for model: \", model_name[0])\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,            # load model in 8-bit precision\n",
        "    bnb_8bit_quant_type=\"nf8\",    # pre-trained model should be quantized in 8-bit NF format\n",
        "    bnb_8bit_use_double_quant=False, # Not Using double quantization as mentioned in QLoRA paper\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name[0],\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pn8KBt3Mq0I",
        "outputId": "d911a96e-1615-4eb5-e202-8caf348d2779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 2,621,440 || all params: 2,778,485,760 || trainable%: 0.09434779323828531\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "\n",
        "# Define LoRA Config\n",
        "lora_config = LoraConfig(\n",
        " r= 8,\n",
        " lora_alpha=16,\n",
        " target_modules=[\"query_key_value\"],\n",
        " lora_dropout=0.05,\n",
        " bias=\"none\",\n",
        " task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "# prepare int-8 model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# add LoRA adaptor\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLhnh5CzQTZf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgkz6-u-PvEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb2793a-ef5b-4de9-8d74-85cee58d2820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
            "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Identify people's perspective on stock market as a reaction to the following context. If you infer that people expect the stock price to go down, start your answer with \"People will have bearish perspective about the stock.\" If you infer that people expect the stock price to go up, start your answer with \"People will have bullish perspective about the stock.\" If you infer that people expect the stock price to stay constant, start your answer with \"People will have neutral perspective about the stock.\" \n",
            "\n",
            "### Input:\n",
            "\n",
            "        context: U.K.’s Largest Pension Fund Plans to Halve Hedge-Fund Holdings\n",
            "        \n",
            "\n",
            "### Response: People will have a neutral perspective about the stock given the headline “U.K.’s Largest Pension Fund Plans to Halve Hedge-Fund Holdings.” This is because from a financial market’s point of view, the impact of this action is largely balanced. \n",
            "\n",
            "Let's first understand the active participants here, the U.K.’s Largest Pension Fund and Hedge Funds. Pension funds are traditionally conservative investors that assemble vast pools of money to guarantee payments to employees once they retire. Hedge funds, on the other hand, are typically more aggressive in their investment strategies, often using leverage and derivatives in order to maximize returns. \n",
            "\n",
            "To start with, when a pension fund reduces its exposure to hedge funds, it lessens its own volatility in the portfolio. Given that hedge funds often involve higher-risk strategies, a scaling back of such holdings could potentially reduce potential losses in scenarios of market volatility. From this viewpoint, such a move stands to fortify the overall stability of the investment ecosystem, which the market generally perceives with neutrality or positivity.\n",
            "\n",
            "However, on the flip side, the reduction of a significant investor's involvement in hedge funds – in this case, the U.K.'s largest pension fund – can also lead to a decrease in the demand for hedge fund-related stocks. This may have a downward pressure on the prices of these stocks. \n",
            "\n",
            "That said, keep in mind the principle of risk and reward. While the hedge funds offer the potential for greater rewards, they do carry higher risk. So the pension fund's decision to halve its hedge fund investments could also mean a lower potential for both upside and downside, meaning less fluctuation, or volatility, in those associated stocks. \n",
            "\n",
            "The above factors are likely to balance out, with the reduced demand applying downward pressure, and the reduced exposure to high-risk investments steadying the market, hence the expectation of stock prices remaining relatively constant. Given these expectations, market participants, coinciding with this sentiment, will likely hold their current positions, waiting for further clarity on future moves before initiating any significant transactions. The anticipation of stability encourages them to take no action, thereby maintaining the status quo and contributing to the neutral effect on stock prices.<eos>\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "eval_steps = 100\n",
        "save_steps = 100\n",
        "logging_steps = 20\n",
        "#optim = \"paged_adamw_32bit\"\n",
        "\n",
        "split_dataset = load_dataset(dataset, split='train').train_test_split(test_size=0.03865, seed = 42)\n",
        "train_data = split_dataset['train']\n",
        "eval_data = split_dataset['test']\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name[0],add_eos_token=True)\n",
        "tokenizer.pad_token_id = 0\n",
        "tokenizer.add_special_tokens({'eos_token':'<eos>'})\n",
        "\n",
        "def generate_prompt(example: dict) -> str:\n",
        "    \"\"\"Generates a standardized message to prompt the model with an instruction, optional input and a\n",
        "    'response' field.\"\"\"\n",
        "\n",
        "    if example[\"input\"]:\n",
        "        return (\n",
        "            \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "            f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response: {example['output']}\" + \"<eos>\"\n",
        "        )\n",
        "    return (\n",
        "        \"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\" + \"<eos>\"\n",
        "    )\n",
        "\n",
        "print(generate_prompt(train_data[3]))\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset= eval_data,\n",
        "    formatting_func=generate_prompt,\n",
        "    packing=True,\n",
        "    max_seq_length = 2048,\n",
        "    args=transformers.TrainingArguments(\n",
        "        num_train_epochs=5,\n",
        "        learning_rate=3e-4,\n",
        "        logging_steps=logging_steps,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        eval_steps=eval_steps,\n",
        "        save_steps=save_steps,\n",
        "        output_dir=output_dir,\n",
        "        report_to=report_to if report_to else \"none\",\n",
        "        save_total_limit=3,\n",
        "        load_best_model_at_end=True,\n",
        "        push_to_hub=False,\n",
        "        auto_find_batch_size=True,\n",
        "        warmup_steps=100,\n",
        "        weight_decay = 0.01,\n",
        "        per_device_train_batch_size = 2,\n",
        "        per_device_eval_batch_size = 2,\n",
        "        gradient_accumulation_steps = 32\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVixs_h7Q7ta",
        "outputId": "26c17cff-05cf-461b-b826-bb620015e32c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cOYCsxvYyCZ"
      },
      "outputs": [],
      "source": [
        "trainer.model.push_to_hub(repo_id)\n",
        "tokenizer.push_to_hub(repo_id)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}